{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9479ad11-7e97-461c-bf2d-6aaac18e2e70",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas\n",
    "import time\n",
    "import math\n",
    "import random\n",
    "import os\n",
    "import torch\n",
    "import scipy.spatial.distance\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, utils\n",
    "\n",
    "import plotly.graph_objects as go\n",
    "import plotly.express as px\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "random.seed = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e5fe1632-868f-442c-be1d-ec02fd7f280d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'NVIDIA GeForce RTX 4060 Laptop GPU'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.empty_cache()\n",
    "torch.cuda.get_device_name(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4ba0e523-fbe8-4ea3-ac08-6b9f25c5807b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from path import Path\n",
    "path = Path(\"./ModelNet10\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ce64f74c-7068-4c5f-91ab-c228795a193f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'bathtub': 0,\n",
       " 'bed': 1,\n",
       " 'chair': 2,\n",
       " 'desk': 3,\n",
       " 'dresser': 4,\n",
       " 'monitor': 5,\n",
       " 'night_stand': 6,\n",
       " 'sofa': 7,\n",
       " 'table': 8,\n",
       " 'toilet': 9}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "folders = [dir for dir in sorted(os.listdir(path)) if os.path.isdir(path/dir)]\n",
    "classes = {folder: i for i, folder in enumerate(folders)};\n",
    "classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "eab5dcad-10c5-4124-b8e9-fdd54741c22c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_off(file):\n",
    "    # 读取文件头并检查格式\n",
    "    off_header = file.readline().strip()\n",
    "    if off_header != 'OFF':\n",
    "        raise ValueError('Not a valid OFF header')\n",
    "\n",
    "    # 读取顶点和面数量\n",
    "    n_verts, n_faces, _ = map(int, file.readline().strip().split())\n",
    "\n",
    "    # 读取顶点坐标\n",
    "    verts = [list(map(float, file.readline().strip().split())) for _ in range(n_verts)]\n",
    "\n",
    "    # 读取面信息\n",
    "    faces = [list(map(int, file.readline().strip().split()))[1:] for _ in range(n_faces)]\n",
    "\n",
    "    return verts, faces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "936699a9-5087-4f2a-9d1d-e3e3dadc4ea2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PointSampler(object):\n",
    "    def __init__(self, output_size):\n",
    "        assert isinstance(output_size, int)\n",
    "        self.output_size = output_size\n",
    "    \n",
    "    def triangle_area(self, pt1, pt2, pt3):\n",
    "        side_a = np.linalg.norm(pt1 - pt2)\n",
    "        side_b = np.linalg.norm(pt2 - pt3)\n",
    "        side_c = np.linalg.norm(pt3 - pt1)\n",
    "        s = 0.5 * ( side_a + side_b + side_c)\n",
    "        return max(s * (s - side_a) * (s - side_b) * (s - side_c), 0)**0.5\n",
    "\n",
    "    def sample_point(self, pt1, pt2, pt3):\n",
    "        # barycentric coordinates on a triangle\n",
    "        # https://mathworld.wolfram.com/BarycentricCoordinates.html\n",
    "        s, t = sorted([random.random(), random.random()])\n",
    "        f = lambda i: s * pt1[i] + (t-s)*pt2[i] + (1-t)*pt3[i]\n",
    "        return (f(0), f(1), f(2))\n",
    "        \n",
    "    \n",
    "    def __call__(self, mesh):\n",
    "        verts, faces = mesh\n",
    "        verts = np.array(verts)\n",
    "        areas = np.zeros((len(faces)))\n",
    "\n",
    "        for i in range(len(areas)):\n",
    "            areas[i] = (self.triangle_area(verts[faces[i][0]],\n",
    "                                           verts[faces[i][1]],\n",
    "                                           verts[faces[i][2]]))\n",
    "            \n",
    "        sampled_faces = (random.choices(faces, \n",
    "                                      weights=areas,\n",
    "                                      cum_weights=None,\n",
    "                                      k=self.output_size))\n",
    "        \n",
    "        sampled_points = np.zeros((self.output_size, 3))\n",
    "\n",
    "        for i in range(len(sampled_faces)):\n",
    "            sampled_points[i] = (self.sample_point(verts[sampled_faces[i][0]],\n",
    "                                                   verts[sampled_faces[i][1]],\n",
    "                                                   verts[sampled_faces[i][2]]))\n",
    "        \n",
    "        return sampled_points\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1d23792e-ea15-4af2-8a5d-e83edec69b7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Normalize(object):\n",
    "    def __call__(self, pointcloud):\n",
    "        assert len(pointcloud.shape)==2\n",
    "        \n",
    "        norm_pointcloud = pointcloud - np.mean(pointcloud, axis=0) \n",
    "        norm_pointcloud /= np.max(np.linalg.norm(norm_pointcloud, axis=1))\n",
    "\n",
    "        return  norm_pointcloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "039890db-372b-4dd1-b368-a1e2f87fc08e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandRotation_z(object):\n",
    "    def __call__(self, pointcloud):\n",
    "        assert len(pointcloud.shape)==2\n",
    "\n",
    "        theta = random.random() * 2. * math.pi\n",
    "        rot_matrix = np.array([[ math.cos(theta), -math.sin(theta),    0],\n",
    "                               [ math.sin(theta),  math.cos(theta),    0],\n",
    "                               [0,                             0,      1]])\n",
    "        \n",
    "        rot_pointcloud = rot_matrix.dot(pointcloud.T).T\n",
    "        return  rot_pointcloud\n",
    "    \n",
    "class RandomNoise(object):\n",
    "    def __call__(self, pointcloud):\n",
    "        assert len(pointcloud.shape)==2\n",
    "\n",
    "        noise = np.random.normal(0, 0.02, (pointcloud.shape))\n",
    "    \n",
    "        noisy_pointcloud = pointcloud + noise\n",
    "        return  noisy_pointcloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7d93c62a-6a81-4e65-838e-085dd324bd1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ToTensor(object):\n",
    "    def __call__(self, pointcloud):\n",
    "        assert len(pointcloud.shape)==2\n",
    "\n",
    "        return torch.from_numpy(pointcloud)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5f50a816-9258-44f3-8d20-a4866ce37b7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def default_transforms():\n",
    "    return transforms.Compose([\n",
    "                                PointSampler(1024),\n",
    "                                Normalize(),\n",
    "                                ToTensor()\n",
    "                              ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b9269bc3-1fc1-4b3b-968d-637841387ddd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PointCloudData(Dataset):\n",
    "    def __init__(self, root_dir, test=False, folder=\"train\", transform=default_transforms()):\n",
    "        self.root_dir = root_dir\n",
    "        folders = [dir for dir in sorted(os.listdir(root_dir)) if os.path.isdir(root_dir/dir)]\n",
    "        self.classes = {folder: i for i, folder in enumerate(folders)}\n",
    "        self.transforms = transform if not test else default_transforms()\n",
    "        self.test = test\n",
    "        self.files = []\n",
    "        for category in self.classes.keys():\n",
    "            new_dir = root_dir/Path(category)/folder\n",
    "            for file in os.listdir(new_dir):\n",
    "                if file.endswith('.off'):\n",
    "                    sample = {}\n",
    "                    sample['pcd_path'] = new_dir/file\n",
    "                    sample['category'] = category\n",
    "                    self.files.append(sample)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.files)\n",
    "\n",
    "    def __preproc__(self, file):\n",
    "        verts, faces = read_off(file)\n",
    "        if self.transforms:\n",
    "            pointcloud = self.transforms((verts, faces))\n",
    "        return pointcloud\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        pcd_path = self.files[idx]['pcd_path']\n",
    "        category = self.files[idx]['category']\n",
    "        with open(pcd_path, 'r') as f:\n",
    "            pointcloud = self.__preproc__(f)\n",
    "        return {'pointcloud': pointcloud, \n",
    "                'category': self.classes[category]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "86ec3c6e-2c68-4f06-8cae-5721014af8d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_transforms = transforms.Compose([\n",
    "                    PointSampler(1024),\n",
    "                    Normalize(),\n",
    "                    RandRotation_z(),\n",
    "                    RandomNoise(),\n",
    "                    ToTensor()\n",
    "                    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "7da1b380-51c9-4542-bf75-56f64de8defc",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = PointCloudData(path, transform=train_transforms)\n",
    "test_ds = PointCloudData(path, test=True, folder='test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2a696bae-ef01-422d-9379-383b6ab60bcd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataset size:  3991\n",
      "Test dataset size:  908\n"
     ]
    }
   ],
   "source": [
    "print('Train dataset size: ', len(train_ds))\n",
    "print('Test dataset size: ', len(test_ds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7377ad1b-2691-4143-8055-4f8e55f8ec61",
   "metadata": {},
   "outputs": [],
   "source": [
    "#按9比1的分割训练数据集：训练集：9； 验证集：1\n",
    "from torch.utils.data import random_split\n",
    "def train_val_split(train_ds):\n",
    "    train_size = int(len(train_ds) * 0.90)\n",
    "    val_size = len(train_ds) - train_size\n",
    "     \n",
    "    train_ds, val_ds = random_split(train_ds, [train_size, val_size])\n",
    "    return train_ds, val_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f6e7e203-51cb-4c0f-97ab-9df72334498b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds, valid_ds = train_val_split(train_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4ce19e67-6d89-40c7-aed6-db06fc16698a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataset size:  3591\n",
      "Valid dataset size:  400\n"
     ]
    }
   ],
   "source": [
    "print('Train dataset size: ', len(train_ds))\n",
    "print('Valid dataset size: ', len(valid_ds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e113a5ec-a88b-4b98-9160-a9747b893b1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(dataset=train_ds, batch_size=32, shuffle=True)\n",
    "valid_loader = DataLoader(dataset=valid_ds, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "665dc354-d483-4381-bcb0-6b1d5b35ee08",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loader = DataLoader(dataset=test_ds, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "21645787-4250-4953-bc3d-426561d9f383",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "86b2890e-a2b3-417e-bbf6-3f7a71490fcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class Tnet(nn.Module):\n",
    "   def __init__(self, k=3):\n",
    "      super().__init__()\n",
    "      self.k=k\n",
    "      self.conv1 = nn.Conv1d(k,64,1)\n",
    "      self.conv2 = nn.Conv1d(64,128,1)\n",
    "      self.conv3 = nn.Conv1d(128,1024,1)\n",
    "      self.fc1 = nn.Linear(1024,512)\n",
    "      self.fc2 = nn.Linear(512,256)\n",
    "      self.fc3 = nn.Linear(256,k*k)\n",
    "\n",
    "      self.bn1 = nn.BatchNorm1d(64)\n",
    "      self.bn2 = nn.BatchNorm1d(128)\n",
    "      self.bn3 = nn.BatchNorm1d(1024)\n",
    "      self.bn4 = nn.BatchNorm1d(512)\n",
    "      self.bn5 = nn.BatchNorm1d(256)\n",
    "       \n",
    "\n",
    "   def forward(self, input):\n",
    "      # input.shape == (bs,n,3)\n",
    "      bs = input.size(0)\n",
    "      xb = F.relu(self.bn1(self.conv1(input)))\n",
    "      xb = F.relu(self.bn2(self.conv2(xb)))\n",
    "      xb = F.relu(self.bn3(self.conv3(xb)))\n",
    "      pool = nn.MaxPool1d(xb.size(-1))(xb)\n",
    "      flat = nn.Flatten(1)(pool)\n",
    "      xb = F.relu(self.bn4(self.fc1(flat)))\n",
    "      xb = F.relu(self.bn5(self.fc2(xb)))\n",
    "      \n",
    "      #initialize as identity\n",
    "      init = torch.eye(self.k, requires_grad=True).repeat(bs,1,1)\n",
    "      if xb.is_cuda:\n",
    "        init=init.cuda()\n",
    "      matrix = self.fc3(xb).view(-1,self.k,self.k) + init\n",
    "      return matrix\n",
    "\n",
    "\n",
    "class Transform(nn.Module):\n",
    "   def __init__(self):\n",
    "        super().__init__()\n",
    "        self.input_transform = Tnet(k=3)\n",
    "        self.feature_transform = Tnet(k=64)\n",
    "        self.conv1 = nn.Conv1d(3,64,1)\n",
    "\n",
    "        self.conv2 = nn.Conv1d(64,128,1)\n",
    "        self.conv3 = nn.Conv1d(128,1024,1)\n",
    "       \n",
    "\n",
    "        self.bn1 = nn.BatchNorm1d(64)\n",
    "        self.bn2 = nn.BatchNorm1d(128)\n",
    "        self.bn3 = nn.BatchNorm1d(1024)\n",
    "       \n",
    "   def forward(self, input):\n",
    "        matrix3x3 = self.input_transform(input)\n",
    "        # batch matrix multiplication\n",
    "        xb = torch.bmm(torch.transpose(input,1,2), matrix3x3).transpose(1,2)\n",
    "\n",
    "        xb = F.relu(self.bn1(self.conv1(xb)))\n",
    "\n",
    "        matrix64x64 = self.feature_transform(xb)\n",
    "        xb = torch.bmm(torch.transpose(xb,1,2), matrix64x64).transpose(1,2)\n",
    "\n",
    "        xb = F.relu(self.bn2(self.conv2(xb)))\n",
    "        xb = self.bn3(self.conv3(xb))\n",
    "        xb = nn.MaxPool1d(xb.size(-1))(xb)\n",
    "        output = nn.Flatten(1)(xb)\n",
    "        return output, matrix3x3, matrix64x64\n",
    "\n",
    "class PointNet(nn.Module):\n",
    "    def __init__(self, classes = 10):\n",
    "        super().__init__()\n",
    "        self.transform = Transform()\n",
    "        self.fc1 = nn.Linear(1024, 512)\n",
    "        self.fc2 = nn.Linear(512, 256)\n",
    "        self.fc3 = nn.Linear(256, classes)\n",
    "        \n",
    "\n",
    "        self.bn1 = nn.BatchNorm1d(512)\n",
    "        self.bn2 = nn.BatchNorm1d(256)\n",
    "        self.dropout = nn.Dropout(p=0.3)\n",
    "        self.logsoftmax = nn.LogSoftmax(dim=1)\n",
    "\n",
    "    def forward(self, input):\n",
    "        xb, matrix3x3, matrix64x64 = self.transform(input)\n",
    "        xb = F.relu(self.bn1(self.fc1(xb)))\n",
    "        xb = F.relu(self.bn2(self.dropout(self.fc2(xb))))\n",
    "        output = self.fc3(xb)\n",
    "        return self.logsoftmax(output), matrix3x3, matrix64x64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "bd6b0a40-6758-47b5-b581-59d139302f5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pointnetloss(outputs, labels, m3x3, m64x64, alpha=0.0001):\n",
    "    criterion = torch.nn.NLLLoss()\n",
    "    \n",
    "    bs = outputs.size(0)\n",
    "    id3x3 = torch.eye(3, device=outputs.device).repeat(bs, 1, 1)\n",
    "    id64x64 = torch.eye(64, device=outputs.device).repeat(bs, 1, 1)\n",
    "\n",
    "    diff3x3 = id3x3 - torch.bmm(m3x3, m3x3.transpose(1, 2))\n",
    "    diff64x64 = id64x64 - torch.bmm(m64x64, m64x64.transpose(1, 2))\n",
    "\n",
    "    # 计算损失\n",
    "    loss = criterion(outputs, labels) + alpha * (torch.norm(diff3x3) + torch.norm(diff64x64)) / float(bs)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "253d53fe-861a-4728-9e93-a0b96ee38d5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8d188a08-07bc-4918-bf63-54e1cc6fbe53",
   "metadata": {},
   "outputs": [],
   "source": [
    "pointnet = PointNet()\n",
    "pointnet.to(device);pointnet = PointNet()\n",
    "pointnet.to(device);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "44e5bde8-5bb7-4552-b40e-1669d30262d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_path = './pointnet_cls_train/ModelNet10'\n",
    "\n",
    "# 优化器\n",
    "optimizer = torch.optim.Adam(pointnet.parameters(), lr=0.001, weight_decay=1e-4)\n",
    "\n",
    "def train(model, train_loader, val_loader=None, epochs=15, save=False, save_path=save_path):\n",
    "\n",
    "    # 记录训练损失和准确率\n",
    "    train_losses = []\n",
    "    train_accuracies = []\n",
    "    val_accuracies = []\n",
    "    best_val_acc = 0.0  # 初始化最佳验证准确率\n",
    "    scaler = torch.cuda.amp.GradScaler()  # 用于半精度训练\n",
    "\n",
    "    total_start_time = time.time()  # 记录总训练开始时间\n",
    "\n",
    "    for epoch in range(epochs): \n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        epoch_start_time = time.time()  # 记录每个 epoch 的开始时间\n",
    "        correct = total = 0  # 用于计算训练准确率\n",
    "\n",
    "        for i, data in enumerate(train_loader, 0):\n",
    "            inputs, labels = data['pointcloud'].to(device).float(), data['category'].to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # 使用自动混合精度\n",
    "            with torch.cuda.amp.autocast():\n",
    "                outputs, m3x3, m64x64 = model(inputs.transpose(1, 2))\n",
    "                loss = pointnetloss(outputs, labels, m3x3, m64x64)\n",
    "\n",
    "            # 反向传播\n",
    "            scaler.scale(loss).backward()  # 半精度反向传播\n",
    "            scaler.step(optimizer)  # 更新优化器\n",
    "            scaler.update()  # 更新缩放器\n",
    "\n",
    "            # 记录损失\n",
    "            running_loss += loss.item()\n",
    "\n",
    "            # 计算训练准确率\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "            if i % 10 == 9:    # 每10个小批次打印一次\n",
    "                print('[Epoch: %d, Batch: %4d / %4d], loss: %.3f' %\n",
    "                      (epoch + 1, i + 1, len(train_loader), running_loss / 10))\n",
    "                running_loss = 0.0\n",
    "\n",
    "        # 记录每个 epoch 的训练损失和准确率\n",
    "        train_losses.append(running_loss)\n",
    "        train_accuracy = 100. * correct / total\n",
    "        train_accuracies.append(train_accuracy)\n",
    "\n",
    "        # 计算并打印每个 epoch 的训练时间\n",
    "        epoch_time = time.time() - epoch_start_time\n",
    "        print('Epoch %d finished in %.2f seconds. Training Accuracy: %.2f %%' % (epoch + 1, epoch_time, train_accuracy))\n",
    "\n",
    "        # 评估模型\n",
    "        model.eval()\n",
    "        correct = total = 0\n",
    "\n",
    "        # 验证过程\n",
    "        if val_loader:\n",
    "            with torch.no_grad():\n",
    "                for data in val_loader:\n",
    "                    inputs, labels = data['pointcloud'].to(device).float(), data['category'].to(device)\n",
    "                    with torch.cuda.amp.autocast():  # 在验证时也使用半精度\n",
    "                        outputs, __, __ = model(inputs.transpose(1, 2))\n",
    "                        _, predicted = torch.max(outputs.data, 1)\n",
    "                        total += labels.size(0)\n",
    "                        correct += (predicted == labels).sum().item()\n",
    "\n",
    "            val_acc = 100. * correct / total\n",
    "            val_accuracies.append(val_acc)\n",
    "            print('Valid accuracy: %.2f %%' % val_acc)\n",
    "\n",
    "            # 保存最优模型\n",
    "            if val_acc > best_val_acc:  # 如果当前验证准确率更高\n",
    "                best_val_acc = val_acc\n",
    "                if save:\n",
    "                    torch.save(model.state_dict(), f\"{save_path}/best_pointnet10_cls.pth\")\n",
    "                    print('Best model saved with accuracy: %.2f %%' % best_val_acc)\n",
    "\n",
    "    # 计算并打印总训练时间\n",
    "    total_time = time.time() - total_start_time\n",
    "    print('Total training time: %.2f seconds.' % total_time)\n",
    "\n",
    "    # 绘制损失曲线\n",
    "    plt.figure()\n",
    "    plt.plot(train_losses, label='Training Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Training Loss Curve')\n",
    "    plt.legend()\n",
    "    plt.savefig(f\"{save_path}/training_loss_curve.png\")\n",
    "    plt.close()\n",
    "\n",
    "    # 绘制准确率曲线\n",
    "    plt.figure()\n",
    "    plt.plot(train_accuracies, label='Training Accuracy')\n",
    "    if val_loader:\n",
    "        plt.plot(val_accuracies, label='Validation Accuracy')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Accuracy (%)')\n",
    "    plt.title('Training and Validation Accuracy Curve')\n",
    "    plt.legend()\n",
    "    plt.savefig(f\"{save_path}/training_validation_accuracy_curve.png\")\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "9f241836-6831-4d28-b3a5-b88087a001d9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch: 1, Batch:   10 /  113], loss: 0.961\n",
      "[Epoch: 1, Batch:   20 /  113], loss: 1.078\n",
      "[Epoch: 1, Batch:   30 /  113], loss: 0.993\n",
      "[Epoch: 1, Batch:   40 /  113], loss: 0.815\n",
      "[Epoch: 1, Batch:   50 /  113], loss: 0.845\n",
      "[Epoch: 1, Batch:   60 /  113], loss: 0.797\n",
      "[Epoch: 1, Batch:   70 /  113], loss: 0.776\n",
      "[Epoch: 1, Batch:   80 /  113], loss: 0.733\n",
      "[Epoch: 1, Batch:   90 /  113], loss: 0.756\n",
      "[Epoch: 1, Batch:  100 /  113], loss: 0.724\n",
      "[Epoch: 1, Batch:  110 /  113], loss: 0.761\n",
      "Epoch 1 finished in 470.04 seconds. Training Accuracy: 72.01 %\n",
      "Valid accuracy: 72.50 %\n",
      "Best model saved with accuracy: 72.50 %\n",
      "[Epoch: 2, Batch:   10 /  113], loss: 0.687\n",
      "[Epoch: 2, Batch:   20 /  113], loss: 0.771\n",
      "[Epoch: 2, Batch:   30 /  113], loss: 0.672\n",
      "[Epoch: 2, Batch:   40 /  113], loss: 0.603\n",
      "[Epoch: 2, Batch:   50 /  113], loss: 0.640\n",
      "[Epoch: 2, Batch:   60 /  113], loss: 0.731\n",
      "[Epoch: 2, Batch:   70 /  113], loss: 0.717\n",
      "[Epoch: 2, Batch:   80 /  113], loss: 0.701\n",
      "[Epoch: 2, Batch:   90 /  113], loss: 0.742\n",
      "[Epoch: 2, Batch:  100 /  113], loss: 0.615\n",
      "[Epoch: 2, Batch:  110 /  113], loss: 0.631\n",
      "Epoch 2 finished in 473.34 seconds. Training Accuracy: 77.17 %\n",
      "Valid accuracy: 71.25 %\n",
      "[Epoch: 3, Batch:   10 /  113], loss: 0.661\n",
      "[Epoch: 3, Batch:   20 /  113], loss: 0.597\n",
      "[Epoch: 3, Batch:   30 /  113], loss: 0.550\n",
      "[Epoch: 3, Batch:   40 /  113], loss: 0.552\n",
      "[Epoch: 3, Batch:   50 /  113], loss: 0.612\n",
      "[Epoch: 3, Batch:   60 /  113], loss: 0.582\n",
      "[Epoch: 3, Batch:   70 /  113], loss: 0.580\n",
      "[Epoch: 3, Batch:   80 /  113], loss: 0.527\n",
      "[Epoch: 3, Batch:   90 /  113], loss: 0.630\n",
      "[Epoch: 3, Batch:  100 /  113], loss: 0.660\n",
      "[Epoch: 3, Batch:  110 /  113], loss: 0.636\n",
      "Epoch 3 finished in 492.36 seconds. Training Accuracy: 79.78 %\n",
      "Valid accuracy: 72.25 %\n",
      "[Epoch: 4, Batch:   10 /  113], loss: 0.613\n",
      "[Epoch: 4, Batch:   20 /  113], loss: 0.614\n",
      "[Epoch: 4, Batch:   30 /  113], loss: 0.512\n",
      "[Epoch: 4, Batch:   40 /  113], loss: 0.538\n",
      "[Epoch: 4, Batch:   50 /  113], loss: 0.546\n",
      "[Epoch: 4, Batch:   60 /  113], loss: 0.602\n",
      "[Epoch: 4, Batch:   70 /  113], loss: 0.663\n",
      "[Epoch: 4, Batch:   80 /  113], loss: 0.433\n",
      "[Epoch: 4, Batch:   90 /  113], loss: 0.550\n",
      "[Epoch: 4, Batch:  100 /  113], loss: 0.549\n",
      "[Epoch: 4, Batch:  110 /  113], loss: 0.445\n",
      "Epoch 4 finished in 481.62 seconds. Training Accuracy: 81.68 %\n",
      "Valid accuracy: 77.25 %\n",
      "Best model saved with accuracy: 77.25 %\n",
      "[Epoch: 5, Batch:   10 /  113], loss: 0.460\n",
      "[Epoch: 5, Batch:   20 /  113], loss: 0.479\n",
      "[Epoch: 5, Batch:   30 /  113], loss: 0.538\n",
      "[Epoch: 5, Batch:   40 /  113], loss: 0.473\n",
      "[Epoch: 5, Batch:   50 /  113], loss: 0.598\n",
      "[Epoch: 5, Batch:   60 /  113], loss: 0.537\n",
      "[Epoch: 5, Batch:   70 /  113], loss: 0.613\n",
      "[Epoch: 5, Batch:   80 /  113], loss: 0.483\n",
      "[Epoch: 5, Batch:   90 /  113], loss: 0.562\n",
      "[Epoch: 5, Batch:  100 /  113], loss: 0.474\n",
      "[Epoch: 5, Batch:  110 /  113], loss: 0.458\n",
      "Epoch 5 finished in 477.60 seconds. Training Accuracy: 82.09 %\n",
      "Valid accuracy: 82.00 %\n",
      "Best model saved with accuracy: 82.00 %\n",
      "[Epoch: 6, Batch:   10 /  113], loss: 0.415\n",
      "[Epoch: 6, Batch:   20 /  113], loss: 0.530\n",
      "[Epoch: 6, Batch:   30 /  113], loss: 0.489\n",
      "[Epoch: 6, Batch:   40 /  113], loss: 0.512\n",
      "[Epoch: 6, Batch:   50 /  113], loss: 0.529\n",
      "[Epoch: 6, Batch:   60 /  113], loss: 0.574\n",
      "[Epoch: 6, Batch:   70 /  113], loss: 0.475\n",
      "[Epoch: 6, Batch:   80 /  113], loss: 0.536\n",
      "[Epoch: 6, Batch:   90 /  113], loss: 0.419\n",
      "[Epoch: 6, Batch:  100 /  113], loss: 0.467\n",
      "[Epoch: 6, Batch:  110 /  113], loss: 0.438\n",
      "Epoch 6 finished in 474.73 seconds. Training Accuracy: 83.90 %\n",
      "Valid accuracy: 84.50 %\n",
      "Best model saved with accuracy: 84.50 %\n",
      "[Epoch: 7, Batch:   10 /  113], loss: 0.356\n",
      "[Epoch: 7, Batch:   20 /  113], loss: 0.404\n",
      "[Epoch: 7, Batch:   30 /  113], loss: 0.461\n",
      "[Epoch: 7, Batch:   40 /  113], loss: 0.528\n",
      "[Epoch: 7, Batch:   50 /  113], loss: 0.401\n",
      "[Epoch: 7, Batch:   60 /  113], loss: 0.490\n",
      "[Epoch: 7, Batch:   70 /  113], loss: 0.501\n",
      "[Epoch: 7, Batch:   80 /  113], loss: 0.564\n",
      "[Epoch: 7, Batch:   90 /  113], loss: 0.385\n",
      "[Epoch: 7, Batch:  100 /  113], loss: 0.417\n",
      "[Epoch: 7, Batch:  110 /  113], loss: 0.460\n",
      "Epoch 7 finished in 473.43 seconds. Training Accuracy: 84.49 %\n",
      "Valid accuracy: 86.25 %\n",
      "Best model saved with accuracy: 86.25 %\n",
      "[Epoch: 8, Batch:   10 /  113], loss: 0.445\n",
      "[Epoch: 8, Batch:   20 /  113], loss: 0.535\n",
      "[Epoch: 8, Batch:   30 /  113], loss: 0.532\n",
      "[Epoch: 8, Batch:   40 /  113], loss: 0.493\n",
      "[Epoch: 8, Batch:   50 /  113], loss: 0.409\n",
      "[Epoch: 8, Batch:   60 /  113], loss: 0.447\n",
      "[Epoch: 8, Batch:   70 /  113], loss: 0.512\n",
      "[Epoch: 8, Batch:   80 /  113], loss: 0.464\n",
      "[Epoch: 8, Batch:   90 /  113], loss: 0.413\n",
      "[Epoch: 8, Batch:  100 /  113], loss: 0.403\n",
      "[Epoch: 8, Batch:  110 /  113], loss: 0.418\n",
      "Epoch 8 finished in 473.97 seconds. Training Accuracy: 83.74 %\n",
      "Valid accuracy: 81.25 %\n",
      "[Epoch: 9, Batch:   10 /  113], loss: 0.460\n",
      "[Epoch: 9, Batch:   20 /  113], loss: 0.507\n",
      "[Epoch: 9, Batch:   30 /  113], loss: 0.559\n",
      "[Epoch: 9, Batch:   40 /  113], loss: 0.463\n",
      "[Epoch: 9, Batch:   50 /  113], loss: 0.446\n",
      "[Epoch: 9, Batch:   60 /  113], loss: 0.421\n",
      "[Epoch: 9, Batch:   70 /  113], loss: 0.406\n",
      "[Epoch: 9, Batch:   80 /  113], loss: 0.405\n",
      "[Epoch: 9, Batch:   90 /  113], loss: 0.454\n",
      "[Epoch: 9, Batch:  100 /  113], loss: 0.431\n",
      "[Epoch: 9, Batch:  110 /  113], loss: 0.484\n",
      "Epoch 9 finished in 473.66 seconds. Training Accuracy: 84.57 %\n",
      "Valid accuracy: 82.00 %\n",
      "[Epoch: 10, Batch:   10 /  113], loss: 0.524\n",
      "[Epoch: 10, Batch:   20 /  113], loss: 0.464\n",
      "[Epoch: 10, Batch:   30 /  113], loss: 0.523\n",
      "[Epoch: 10, Batch:   40 /  113], loss: 0.457\n",
      "[Epoch: 10, Batch:   50 /  113], loss: 0.402\n",
      "[Epoch: 10, Batch:   60 /  113], loss: 0.447\n",
      "[Epoch: 10, Batch:   70 /  113], loss: 0.401\n",
      "[Epoch: 10, Batch:   80 /  113], loss: 0.413\n",
      "[Epoch: 10, Batch:   90 /  113], loss: 0.337\n",
      "[Epoch: 10, Batch:  100 /  113], loss: 0.385\n",
      "[Epoch: 10, Batch:  110 /  113], loss: 0.345\n",
      "Epoch 10 finished in 479.72 seconds. Training Accuracy: 85.69 %\n",
      "Valid accuracy: 88.25 %\n",
      "Best model saved with accuracy: 88.25 %\n",
      "[Epoch: 11, Batch:   10 /  113], loss: 0.358\n",
      "[Epoch: 11, Batch:   20 /  113], loss: 0.467\n",
      "[Epoch: 11, Batch:   30 /  113], loss: 0.374\n",
      "[Epoch: 11, Batch:   40 /  113], loss: 0.343\n",
      "[Epoch: 11, Batch:   50 /  113], loss: 0.466\n",
      "[Epoch: 11, Batch:   60 /  113], loss: 0.379\n",
      "[Epoch: 11, Batch:   70 /  113], loss: 0.436\n",
      "[Epoch: 11, Batch:   80 /  113], loss: 0.335\n",
      "[Epoch: 11, Batch:   90 /  113], loss: 0.389\n",
      "[Epoch: 11, Batch:  100 /  113], loss: 0.298\n",
      "[Epoch: 11, Batch:  110 /  113], loss: 0.393\n",
      "Epoch 11 finished in 480.63 seconds. Training Accuracy: 87.11 %\n",
      "Valid accuracy: 86.00 %\n",
      "[Epoch: 12, Batch:   10 /  113], loss: 0.403\n",
      "[Epoch: 12, Batch:   20 /  113], loss: 0.337\n",
      "[Epoch: 12, Batch:   30 /  113], loss: 0.468\n",
      "[Epoch: 12, Batch:   40 /  113], loss: 0.343\n",
      "[Epoch: 12, Batch:   50 /  113], loss: 0.393\n",
      "[Epoch: 12, Batch:   60 /  113], loss: 0.415\n",
      "[Epoch: 12, Batch:   70 /  113], loss: 0.391\n",
      "[Epoch: 12, Batch:   80 /  113], loss: 0.380\n",
      "[Epoch: 12, Batch:   90 /  113], loss: 0.313\n",
      "[Epoch: 12, Batch:  100 /  113], loss: 0.335\n",
      "[Epoch: 12, Batch:  110 /  113], loss: 0.358\n",
      "Epoch 12 finished in 476.81 seconds. Training Accuracy: 87.41 %\n",
      "Valid accuracy: 84.50 %\n",
      "[Epoch: 13, Batch:   10 /  113], loss: 0.319\n",
      "[Epoch: 13, Batch:   20 /  113], loss: 0.420\n",
      "[Epoch: 13, Batch:   30 /  113], loss: 0.322\n",
      "[Epoch: 13, Batch:   40 /  113], loss: 0.370\n",
      "[Epoch: 13, Batch:   50 /  113], loss: 0.317\n",
      "[Epoch: 13, Batch:   60 /  113], loss: 0.364\n",
      "[Epoch: 13, Batch:   70 /  113], loss: 0.328\n",
      "[Epoch: 13, Batch:   80 /  113], loss: 0.402\n",
      "[Epoch: 13, Batch:   90 /  113], loss: 0.343\n",
      "[Epoch: 13, Batch:  100 /  113], loss: 0.389\n",
      "[Epoch: 13, Batch:  110 /  113], loss: 0.460\n",
      "Epoch 13 finished in 476.11 seconds. Training Accuracy: 88.30 %\n",
      "Valid accuracy: 90.00 %\n",
      "Best model saved with accuracy: 90.00 %\n",
      "[Epoch: 14, Batch:   10 /  113], loss: 0.469\n",
      "[Epoch: 14, Batch:   20 /  113], loss: 0.433\n",
      "[Epoch: 14, Batch:   30 /  113], loss: 0.375\n",
      "[Epoch: 14, Batch:   40 /  113], loss: 0.387\n",
      "[Epoch: 14, Batch:   50 /  113], loss: 0.417\n",
      "[Epoch: 14, Batch:   60 /  113], loss: 0.413\n",
      "[Epoch: 14, Batch:   70 /  113], loss: 0.398\n",
      "[Epoch: 14, Batch:   80 /  113], loss: 0.341\n",
      "[Epoch: 14, Batch:   90 /  113], loss: 0.271\n",
      "[Epoch: 14, Batch:  100 /  113], loss: 0.304\n",
      "[Epoch: 14, Batch:  110 /  113], loss: 0.461\n",
      "Epoch 14 finished in 481.24 seconds. Training Accuracy: 86.44 %\n",
      "Valid accuracy: 84.50 %\n",
      "[Epoch: 15, Batch:   10 /  113], loss: 0.488\n",
      "[Epoch: 15, Batch:   20 /  113], loss: 0.391\n",
      "[Epoch: 15, Batch:   30 /  113], loss: 0.364\n",
      "[Epoch: 15, Batch:   40 /  113], loss: 0.290\n",
      "[Epoch: 15, Batch:   50 /  113], loss: 0.370\n",
      "[Epoch: 15, Batch:   60 /  113], loss: 0.328\n",
      "[Epoch: 15, Batch:   70 /  113], loss: 0.349\n",
      "[Epoch: 15, Batch:   80 /  113], loss: 0.411\n",
      "[Epoch: 15, Batch:   90 /  113], loss: 0.336\n",
      "[Epoch: 15, Batch:  100 /  113], loss: 0.392\n",
      "[Epoch: 15, Batch:  110 /  113], loss: 0.308\n",
      "Epoch 15 finished in 479.48 seconds. Training Accuracy: 87.39 %\n",
      "Valid accuracy: 87.75 %\n",
      "Total training time: 7982.52 seconds.\n"
     ]
    }
   ],
   "source": [
    "train(pointnet, train_loader, valid_loader, save=True, save_path=save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "9401a46d-f37c-46ab-9834-0025bd9427e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'NVIDIA GeForce RTX 4060 Laptop GPU'"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.empty_cache()\n",
    "torch.cuda.get_device_name(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ab7b4ed-42c8-4bcf-aff5-6cd96a98b47a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5004aed5-2c79-473c-bf43-b77353e9250b",
   "metadata": {},
   "source": [
    "## 优化部分"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "d23e584f-46bc-46bb-ba59-aa737d3b697e",
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_train_path = './pointnet_cls_train/ModelNet10/best_pointnet10_cls.pth'\n",
    "pointnet = PointNet()\n",
    "pointnet.load_state_dict(torch.load(pre_train_path))\n",
    "pointnet.eval();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "fea6ed30-c700-4dce-a87f-2f7d21ab026e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch [   1 /   15]\n",
      "Batch [   2 /   15]\n",
      "Batch [   3 /   15]\n",
      "Batch [   4 /   15]\n",
      "Batch [   5 /   15]\n",
      "Batch [   6 /   15]\n",
      "Batch [   7 /   15]\n",
      "Batch [   8 /   15]\n",
      "Batch [   9 /   15]\n",
      "Batch [  10 /   15]\n",
      "Batch [  11 /   15]\n",
      "Batch [  12 /   15]\n",
      "Batch [  13 /   15]\n",
      "Batch [  14 /   15]\n",
      "Batch [  15 /   15]\n",
      "Class 0: Total = 50, Correct = 41, Accuracy = 0.8200\n",
      "Class 1: Total = 100, Correct = 84, Accuracy = 0.8400\n",
      "Class 2: Total = 100, Correct = 93, Accuracy = 0.9300\n",
      "Class 3: Total = 86, Correct = 58, Accuracy = 0.6744\n",
      "Class 4: Total = 86, Correct = 66, Accuracy = 0.7674\n",
      "Class 5: Total = 100, Correct = 94, Accuracy = 0.9400\n",
      "Class 6: Total = 86, Correct = 60, Accuracy = 0.6977\n",
      "Class 7: Total = 100, Correct = 94, Accuracy = 0.9400\n",
      "Class 8: Total = 100, Correct = 89, Accuracy = 0.8900\n",
      "Class 9: Total = 100, Correct = 88, Accuracy = 0.8800\n",
      "Overall Accuracy: 0.8447\n"
     ]
    }
   ],
   "source": [
    "# 类别数量\n",
    "num_classes = 10 \n",
    "\n",
    "# 用于存储每类的预测和真实标签\n",
    "class_correct = [0] * num_classes\n",
    "class_total = [0] * num_classes\n",
    "\n",
    "# 全局统计\n",
    "total_correct = 0\n",
    "total_samples = 0\n",
    "\n",
    "all_preds = []\n",
    "all_labels = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for i, data in enumerate(test_loader):\n",
    "        print('Batch [%4d / %4d]' % (i + 1, len(test_loader)))\n",
    "\n",
    "        inputs, labels = data['pointcloud'].float(), data['category']\n",
    "        outputs, __, __ = pointnet(inputs.transpose(1, 2))\n",
    "        \n",
    "        # 获取预测结果\n",
    "        _, preds = torch.max(outputs.data, 1)\n",
    "        \n",
    "        # 更新所有预测和标签\n",
    "        all_preds += list(preds.numpy())\n",
    "        all_labels += list(labels.numpy())\n",
    "\n",
    "        # 统计每个类别的正确预测数和总数\n",
    "        for j in range(labels.size(0)):\n",
    "            label = labels[j].item()  # 获取当前样本的真实标签\n",
    "            class_total[label] += 1  # 该类样本总数加 1\n",
    "            class_correct[label] += (preds[j] == label).item()  # 如果预测正确，相应类别的正确预测数加 1\n",
    "            \n",
    "            # 更新全局统计\n",
    "            total_samples += 1\n",
    "            total_correct += (preds[j] == label).item()\n",
    "\n",
    "# 计算每类的准确率\n",
    "class_accuracy = [0] * num_classes\n",
    "for i in range(num_classes):\n",
    "    if class_total[i] > 0:\n",
    "        class_accuracy[i] = class_correct[i] / class_total[i]\n",
    "    else:\n",
    "        class_accuracy[i] = 0.0  # 如果该类没有样本，准确率为 0\n",
    "\n",
    "# 计算总体分类准确率\n",
    "overall_accuracy = total_correct / total_samples if total_samples > 0 else 0.0\n",
    "\n",
    "# 打印每类的测试数量和准确率\n",
    "for i in range(num_classes):\n",
    "    print(f'Class {i}: Total = {class_total[i]}, Correct = {class_correct[i]}, Accuracy = {class_accuracy[i]:.4f}')\n",
    "\n",
    "# 打印总体分类准确率\n",
    "print(f'Overall Accuracy: {overall_accuracy:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "e785ead0-2526-424f-81b0-77722b92f493",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[50, 100, 100, 86, 86, 100, 86, 100, 100, 100]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "a2320a7a-1027-4680-9493-a45a29308530",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculated class weights: tensor([0.0638, 0.1135, 0.0496, 0.1986, 0.1418, 0.0426, 0.1844, 0.0426, 0.0780,\n",
      "        0.0851])\n"
     ]
    }
   ],
   "source": [
    "# 计算类别权重\n",
    "inverse_accuracy = 1 - np.array(class_accuracy)  # 反向准确率\n",
    "weights = inverse_accuracy * class_total  # 结合样本数量\n",
    "\n",
    "# 归一化权重\n",
    "weights /= np.sum(weights)  # 使总和为1\n",
    "\n",
    "# 转换为PyTorch张量\n",
    "class_weights = torch.tensor(weights, dtype=torch.float)\n",
    "\n",
    "print(\"Calculated class weights:\", class_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "53a0607c-a5c3-47f8-88e1-91ad13ee719b",
   "metadata": {},
   "outputs": [],
   "source": [
    "### 引入新的数据增强方法\n",
    "\n",
    "def generate_sphere(center, radius, num_points):\n",
    "    \"\"\"生成一个球体的点云\"\"\"\n",
    "    u = np.random.uniform(0, 1, num_points)\n",
    "    v = np.random.uniform(0, 1, num_points)\n",
    "    \n",
    "    theta = 2 * np.pi * u\n",
    "    phi = np.arccos(2 * v - 1)\n",
    "\n",
    "    # 根据球坐标系转换为三维笛卡尔坐标系\n",
    "    x = center[0] + radius * np.sin(phi) * np.cos(theta)\n",
    "    y = center[1] + radius * np.sin(phi) * np.sin(theta)\n",
    "    z = center[2] + radius * np.cos(phi)\n",
    "\n",
    "    return np.column_stack((x, y, z))\n",
    "\n",
    "def apply_occlusion(pointcloud, sphere_center, sphere_radius):\n",
    "    \"\"\"将生成的球体和原本的点云数据合成，将球体内的点云数据删除\"\"\"\n",
    "    distances = np.linalg.norm(pointcloud - sphere_center, axis=1)\n",
    "    occluded_pointcloud = pointcloud[distances > sphere_radius]\n",
    "    return occluded_pointcloud\n",
    "\n",
    "def pad_or_crop(pointcloud, target_size):\n",
    "    \"\"\"填充或裁剪点云到目标大小\"\"\"\n",
    "    current_size = pointcloud.shape[0]\n",
    "    if current_size < target_size:\n",
    "        padding = np.zeros((target_size - current_size, pointcloud.shape[1]), dtype=pointcloud.dtype)\n",
    "        return np.vstack((pointcloud, padding))  # 在第一个维度上堆叠\n",
    "    elif current_size > target_size:\n",
    "        return pointcloud[:target_size]  # 裁剪到目标大小\n",
    "    return pointcloud\n",
    "\n",
    "class Sphere_Occlusion(object):\n",
    "    def __call__(self, pointcloud):\n",
    "        assert len(pointcloud.shape) == 2\n",
    "        random_index = np.random.randint(0, pointcloud.shape[0])\n",
    "        sphere_center = pointcloud[random_index]\n",
    "        \n",
    "        sphere_radius = 0.2\n",
    "        num_sphere_points = 400\n",
    "\n",
    "        sphere_pointcloud = generate_sphere(sphere_center, sphere_radius, num_sphere_points)\n",
    "        \n",
    "        occluded_pointcloud = apply_occlusion(pointcloud, sphere_center, sphere_radius)\n",
    "\n",
    "        # 确保输出的点云大小为 [1024, 3]\n",
    "        return pad_or_crop(occluded_pointcloud, 1024)\n",
    "\n",
    "class RandomShift(object):\n",
    "    def __call__(self, pointcloud):\n",
    "        assert len(pointcloud.shape) == 2\n",
    "        shift_range = 0.1\n",
    "\n",
    "        shifts = np.random.uniform(-shift_range, shift_range, pointcloud.shape)\n",
    "        shifted_pointcloud = pointcloud + shifts\n",
    "        \n",
    "        # 确保输出的点云大小为 [1024, 3]\n",
    "        return pad_or_crop(shifted_pointcloud, 1024)\n",
    "\n",
    "class RandomScale(object):\n",
    "    def __call__(self, pointcloud):\n",
    "        assert len(pointcloud.shape) == 2\n",
    "        scale_low = 0.8\n",
    "        scale_high = 1.25\n",
    "        scales = np.random.uniform(scale_low, scale_high, (pointcloud.shape[0], 1))\n",
    "        scaled_pointcloud = pointcloud * scales\n",
    "        \n",
    "        # 确保输出的点云大小为 [1024, 3]\n",
    "        return pad_or_crop(scaled_pointcloud, 1024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "5c0acf97-0ce3-42a4-8c83-c9e08349ee72",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_transforms = transforms.Compose([\n",
    "                    PointSampler(1024),\n",
    "                    Normalize(),\n",
    "                    Sphere_Occlusion(),\n",
    "                    RandomShift(),\n",
    "                    RandomScale(),\n",
    "                    ToTensor()\n",
    "                    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "efb70a62-f21b-4fab-a2ce-70f26d46dc75",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = PointCloudData(path, transform=train_transforms)\n",
    "test_ds = PointCloudData(path, test=True, folder='test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "fdabcc9f-db24-42f8-8479-5dfe1c9f480f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All tensors have the same shape: torch.Size([1024, 3])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def check_tensor_shapes(dataset):\n",
    "    # 获取第一个样本的形状作为基准\n",
    "    first_shape = None\n",
    "\n",
    "    for i, data in enumerate(dataset):\n",
    "        # 假设每个样本是一个字典，包含 'pointcloud' 和 'category'\n",
    "        tensor = data['pointcloud']  # 替换为您的张量键\n",
    "\n",
    "        # 检查张量的形状\n",
    "        if first_shape is None:\n",
    "            first_shape = tensor.shape\n",
    "        else:\n",
    "            if tensor.shape != first_shape:\n",
    "                print(f\"Mismatch found in sample {i}: {tensor.shape} vs {first_shape}\")\n",
    "                return False\n",
    "\n",
    "    print(\"All tensors have the same shape:\", first_shape)\n",
    "    return True\n",
    "\n",
    "# 使用示例\n",
    "check_tensor_shapes(train_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "f80594b9-cf41-489a-bf26-620783c0c267",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataset size:  3591\n",
      "Test dataset size:  908\n"
     ]
    }
   ],
   "source": [
    "print('Train dataset size: ', len(train_ds))\n",
    "print('Test dataset size: ', len(test_ds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "055f0686-76ac-4d88-a7cb-2ba92940b6f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#按9比1的分割训练数据集：训练集：9； 验证集：1\n",
    "from torch.utils.data import random_split\n",
    "def train_val_split(train_ds):\n",
    "    train_size = int(len(train_ds) * 0.90)\n",
    "    val_size = len(train_ds) - train_size\n",
    "     \n",
    "    train_ds, val_ds = random_split(train_ds, [train_size, val_size])\n",
    "    return train_ds, val_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "d5fb6511-e806-413e-a97d-79382a1c3180",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds, valid_ds = train_val_split(train_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "e5265f38-0a0a-40a0-9d66-cb40795b38cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(dataset=train_ds, batch_size=32, shuffle=True)\n",
    "valid_loader = DataLoader(dataset=valid_ds, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "169360cf-1b58-4dba-8614-8fc0a532633c",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loader = DataLoader(dataset=test_ds, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "1613fb1a-d98b-4ff7-8c0b-3a94463eea00",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pointnetloss(outputs, labels, m3x3, m64x64, class_weights=None, alpha=0.0001):\n",
    "    # Convert outputs to log probabilities if they are logits\n",
    "    log_probs = torch.nn.functional.log_softmax(outputs, dim=1)\n",
    "\n",
    "    if class_weights is not None:\n",
    "        class_weights = class_weights.to(outputs.device)\n",
    "\n",
    "    # Use weighted negative log likelihood loss\n",
    "    criterion = torch.nn.NLLLoss(weight=class_weights) if class_weights is not None else torch.nn.NLLLoss()\n",
    "    \n",
    "    bs = outputs.size(0)  # Get batch size\n",
    "\n",
    "    # Create identity matrices\n",
    "    id3x3 = torch.eye(3, device=outputs.device).repeat(bs, 1, 1)\n",
    "    id64x64 = torch.eye(64, device=outputs.device).repeat(bs, 1, 1)\n",
    "\n",
    "    # Calculate differences\n",
    "    diff3x3 = id3x3 - torch.bmm(m3x3, m3x3.transpose(1, 2))\n",
    "    diff64x64 = id64x64 - torch.bmm(m64x64, m64x64.transpose(1, 2))\n",
    "\n",
    "    # Calculate total loss\n",
    "    loss = criterion(log_probs, labels) + alpha * (torch.norm(diff3x3) + torch.norm(diff64x64)) / float(bs)\n",
    "    \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "7b971324-1f0f-412a-aec5-dc0cd9b0ff22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "e097061a-7ac9-46ff-9a57-dc3a276cbfa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "pointnet.load_state_dict(torch.load(pre_train_path))\n",
    "pointnet = PointNet()\n",
    "pointnet.to(device);pointnet = PointNet()\n",
    "pointnet.to(device);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "b44fe449-0427-4a5b-bf8b-f0acca2ece1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(pointnet.parameters(), lr=0.001, weight_decay=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "eb7e484e-9d50-428a-8136-5e84c269175a",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_path = './pointnet_cls_train/ModelNet10'\n",
    "\n",
    "def train(model, train_loader, val_loader=None, epochs=15, class_weights=None, save=False, save_path=save_path):\n",
    "    \n",
    "    # 记录训练损失和准确率\n",
    "    train_losses = []\n",
    "    train_accuracies = []\n",
    "    val_accuracies = []\n",
    "    best_val_acc = 0.0  # 初始化最佳验证准确率\n",
    "    scaler = torch.cuda.amp.GradScaler()  # 用于半精度训练\n",
    "\n",
    "    total_start_time = time.time()  # 记录总训练开始时间\n",
    "\n",
    "    for epoch in range(epochs): \n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        epoch_start_time = time.time()  # 记录每个 epoch 的开始时间\n",
    "        correct = total = 0  # 用于计算训练准确率\n",
    "\n",
    "        for i, data in enumerate(train_loader, 0):\n",
    "            inputs, labels = data['pointcloud'].to(device).float(), data['category'].to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # 使用自动混合精度\n",
    "            with torch.cuda.amp.autocast():\n",
    "                outputs, m3x3, m64x64 = model(inputs.transpose(1, 2))\n",
    "                loss = pointnetloss(outputs, labels, m3x3, m64x64, class_weights)\n",
    "\n",
    "            # 反向传播\n",
    "            scaler.scale(loss).backward()  # 半精度反向传播\n",
    "            scaler.step(optimizer)  # 更新优化器\n",
    "            scaler.update()  # 更新缩放器\n",
    "\n",
    "            # 记录损失\n",
    "            running_loss += loss.item()\n",
    "\n",
    "            # 计算训练准确率\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "            if i % 10 == 9:    # 每10个小批次打印一次\n",
    "                print('[Epoch: %d, Batch: %4d / %4d], loss: %.3f' %\n",
    "                      (epoch + 1, i + 1, len(train_loader), running_loss / 10))\n",
    "                running_loss = 0.0\n",
    "\n",
    "        # 记录每个 epoch 的训练损失和准确率\n",
    "        train_losses.append(running_loss)\n",
    "        train_accuracy = 100. * correct / total\n",
    "        train_accuracies.append(train_accuracy)\n",
    "\n",
    "        # 计算并打印每个 epoch 的训练时间\n",
    "        epoch_time = time.time() - epoch_start_time\n",
    "        print('Epoch %d finished in %.2f seconds. Training Accuracy: %.2f %%' % (epoch + 1, epoch_time, train_accuracy))\n",
    "\n",
    "        # 评估模型\n",
    "        model.eval()\n",
    "        correct = total = 0\n",
    "\n",
    "        # 验证过程\n",
    "        if val_loader:\n",
    "            with torch.no_grad():\n",
    "                for data in val_loader:\n",
    "                    inputs, labels = data['pointcloud'].to(device).float(), data['category'].to(device)\n",
    "                    with torch.cuda.amp.autocast():  # 在验证时也使用半精度\n",
    "                        outputs, __, __ = model(inputs.transpose(1, 2))\n",
    "                        _, predicted = torch.max(outputs.data, 1)\n",
    "                        total += labels.size(0)\n",
    "                        correct += (predicted == labels).sum().item()\n",
    "\n",
    "            val_acc = 100. * correct / total\n",
    "            val_accuracies.append(val_acc)\n",
    "            print('Valid accuracy: %.2f %%' % val_acc)\n",
    "\n",
    "            # 保存最优模型\n",
    "            if val_acc > best_val_acc:  # 如果当前验证准确率更高\n",
    "                best_val_acc = val_acc\n",
    "                if save:\n",
    "                    torch.save(model.state_dict(), f\"{save_path}/best_pointnet10_cls_opt1.pth\")\n",
    "                    print('Best model saved with accuracy: %.2f %%' % best_val_acc)\n",
    "\n",
    "    # 计算并打印总训练时间\n",
    "    total_time = time.time() - total_start_time\n",
    "    print('Total training time: %.2f seconds.' % total_time)\n",
    "\n",
    "    # 绘制损失曲线\n",
    "    plt.figure()\n",
    "    plt.plot(train_losses, label='Training Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Training Loss Curve')\n",
    "    plt.legend()\n",
    "    plt.savefig(f\"{save_path}/training_loss_curve_opt1.png\")\n",
    "    plt.close()\n",
    "\n",
    "    # 绘制准确率曲线\n",
    "    plt.figure()\n",
    "    plt.plot(train_accuracies, label='Training Accuracy')\n",
    "    if val_loader:\n",
    "        plt.plot(val_accuracies, label='Validation Accuracy')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Accuracy (%)')\n",
    "    plt.title('Training and Validation Accuracy Curve')\n",
    "    plt.legend()\n",
    "    plt.savefig(f\"{save_path}/training_validation_accuracy_curve_opt1.png\")\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "a023d8a2-a493-4412-b5bc-0aec435a6d53",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch: 1, Batch:   10 /  113], loss: 2.161\n",
      "[Epoch: 1, Batch:   20 /  113], loss: 1.725\n",
      "[Epoch: 1, Batch:   30 /  113], loss: 1.311\n",
      "[Epoch: 1, Batch:   40 /  113], loss: 1.248\n",
      "[Epoch: 1, Batch:   50 /  113], loss: 1.279\n",
      "[Epoch: 1, Batch:   60 /  113], loss: 1.101\n",
      "[Epoch: 1, Batch:   70 /  113], loss: 0.967\n",
      "[Epoch: 1, Batch:   80 /  113], loss: 0.963\n",
      "[Epoch: 1, Batch:   90 /  113], loss: 0.780\n",
      "[Epoch: 1, Batch:  100 /  113], loss: 0.884\n",
      "[Epoch: 1, Batch:  110 /  113], loss: 0.776\n",
      "Epoch 1 finished in 409.39 seconds. Training Accuracy: 62.38 %\n",
      "Valid accuracy: 64.00 %\n",
      "Best model saved with accuracy: 64.00 %\n",
      "[Epoch: 2, Batch:   10 /  113], loss: 0.791\n",
      "[Epoch: 2, Batch:   20 /  113], loss: 0.688\n",
      "[Epoch: 2, Batch:   30 /  113], loss: 0.746\n",
      "[Epoch: 2, Batch:   40 /  113], loss: 0.851\n",
      "[Epoch: 2, Batch:   50 /  113], loss: 0.747\n",
      "[Epoch: 2, Batch:   60 /  113], loss: 0.780\n",
      "[Epoch: 2, Batch:   70 /  113], loss: 0.840\n",
      "[Epoch: 2, Batch:   80 /  113], loss: 0.695\n",
      "[Epoch: 2, Batch:   90 /  113], loss: 0.696\n",
      "[Epoch: 2, Batch:  100 /  113], loss: 0.634\n",
      "[Epoch: 2, Batch:  110 /  113], loss: 0.670\n",
      "Epoch 2 finished in 411.61 seconds. Training Accuracy: 79.11 %\n",
      "Valid accuracy: 81.75 %\n",
      "Best model saved with accuracy: 81.75 %\n",
      "[Epoch: 3, Batch:   10 /  113], loss: 0.732\n",
      "[Epoch: 3, Batch:   20 /  113], loss: 0.700\n",
      "[Epoch: 3, Batch:   30 /  113], loss: 0.727\n",
      "[Epoch: 3, Batch:   40 /  113], loss: 0.565\n",
      "[Epoch: 3, Batch:   50 /  113], loss: 0.545\n",
      "[Epoch: 3, Batch:   60 /  113], loss: 0.591\n",
      "[Epoch: 3, Batch:   70 /  113], loss: 0.588\n",
      "[Epoch: 3, Batch:   80 /  113], loss: 0.566\n",
      "[Epoch: 3, Batch:   90 /  113], loss: 0.619\n",
      "[Epoch: 3, Batch:  100 /  113], loss: 0.570\n",
      "[Epoch: 3, Batch:  110 /  113], loss: 0.678\n",
      "Epoch 3 finished in 415.48 seconds. Training Accuracy: 84.02 %\n",
      "Valid accuracy: 81.50 %\n",
      "[Epoch: 4, Batch:   10 /  113], loss: 0.729\n",
      "[Epoch: 4, Batch:   20 /  113], loss: 0.562\n",
      "[Epoch: 4, Batch:   30 /  113], loss: 0.622\n",
      "[Epoch: 4, Batch:   40 /  113], loss: 0.590\n",
      "[Epoch: 4, Batch:   50 /  113], loss: 0.613\n",
      "[Epoch: 4, Batch:   60 /  113], loss: 0.404\n",
      "[Epoch: 4, Batch:   70 /  113], loss: 0.582\n",
      "[Epoch: 4, Batch:   80 /  113], loss: 0.421\n",
      "[Epoch: 4, Batch:   90 /  113], loss: 0.462\n",
      "[Epoch: 4, Batch:  100 /  113], loss: 0.444\n",
      "[Epoch: 4, Batch:  110 /  113], loss: 0.532\n",
      "Epoch 4 finished in 410.92 seconds. Training Accuracy: 85.69 %\n",
      "Valid accuracy: 70.00 %\n",
      "[Epoch: 5, Batch:   10 /  113], loss: 0.652\n",
      "[Epoch: 5, Batch:   20 /  113], loss: 0.703\n",
      "[Epoch: 5, Batch:   30 /  113], loss: 0.487\n",
      "[Epoch: 5, Batch:   40 /  113], loss: 0.584\n",
      "[Epoch: 5, Batch:   50 /  113], loss: 0.457\n",
      "[Epoch: 5, Batch:   60 /  113], loss: 0.434\n",
      "[Epoch: 5, Batch:   70 /  113], loss: 0.485\n",
      "[Epoch: 5, Batch:   80 /  113], loss: 0.390\n",
      "[Epoch: 5, Batch:   90 /  113], loss: 0.551\n",
      "[Epoch: 5, Batch:  100 /  113], loss: 0.666\n",
      "[Epoch: 5, Batch:  110 /  113], loss: 0.619\n",
      "Epoch 5 finished in 413.36 seconds. Training Accuracy: 86.52 %\n",
      "Valid accuracy: 89.50 %\n",
      "Best model saved with accuracy: 89.50 %\n",
      "[Epoch: 6, Batch:   10 /  113], loss: 0.521\n",
      "[Epoch: 6, Batch:   20 /  113], loss: 0.485\n",
      "[Epoch: 6, Batch:   30 /  113], loss: 0.387\n",
      "[Epoch: 6, Batch:   40 /  113], loss: 0.520\n",
      "[Epoch: 6, Batch:   50 /  113], loss: 0.439\n",
      "[Epoch: 6, Batch:   60 /  113], loss: 0.575\n",
      "[Epoch: 6, Batch:   70 /  113], loss: 0.469\n",
      "[Epoch: 6, Batch:   80 /  113], loss: 0.499\n",
      "[Epoch: 6, Batch:   90 /  113], loss: 0.469\n",
      "[Epoch: 6, Batch:  100 /  113], loss: 0.525\n",
      "[Epoch: 6, Batch:  110 /  113], loss: 0.597\n",
      "Epoch 6 finished in 414.05 seconds. Training Accuracy: 87.19 %\n",
      "Valid accuracy: 85.00 %\n",
      "[Epoch: 7, Batch:   10 /  113], loss: 0.525\n",
      "[Epoch: 7, Batch:   20 /  113], loss: 0.401\n",
      "[Epoch: 7, Batch:   30 /  113], loss: 0.383\n",
      "[Epoch: 7, Batch:   40 /  113], loss: 0.467\n",
      "[Epoch: 7, Batch:   50 /  113], loss: 0.637\n",
      "[Epoch: 7, Batch:   60 /  113], loss: 0.484\n",
      "[Epoch: 7, Batch:   70 /  113], loss: 0.426\n",
      "[Epoch: 7, Batch:   80 /  113], loss: 0.281\n",
      "[Epoch: 7, Batch:   90 /  113], loss: 0.420\n",
      "[Epoch: 7, Batch:  100 /  113], loss: 0.489\n",
      "[Epoch: 7, Batch:  110 /  113], loss: 0.425\n",
      "Epoch 7 finished in 414.87 seconds. Training Accuracy: 88.97 %\n",
      "Valid accuracy: 83.75 %\n",
      "[Epoch: 8, Batch:   10 /  113], loss: 0.386\n",
      "[Epoch: 8, Batch:   20 /  113], loss: 0.431\n",
      "[Epoch: 8, Batch:   30 /  113], loss: 0.432\n",
      "[Epoch: 8, Batch:   40 /  113], loss: 0.474\n",
      "[Epoch: 8, Batch:   50 /  113], loss: 0.424\n",
      "[Epoch: 8, Batch:   60 /  113], loss: 0.353\n",
      "[Epoch: 8, Batch:   70 /  113], loss: 0.359\n",
      "[Epoch: 8, Batch:   80 /  113], loss: 0.462\n",
      "[Epoch: 8, Batch:   90 /  113], loss: 0.565\n",
      "[Epoch: 8, Batch:  100 /  113], loss: 0.325\n",
      "[Epoch: 8, Batch:  110 /  113], loss: 0.386\n",
      "Epoch 8 finished in 415.15 seconds. Training Accuracy: 89.25 %\n",
      "Valid accuracy: 84.75 %\n",
      "[Epoch: 9, Batch:   10 /  113], loss: 0.332\n",
      "[Epoch: 9, Batch:   20 /  113], loss: 0.297\n",
      "[Epoch: 9, Batch:   30 /  113], loss: 0.388\n",
      "[Epoch: 9, Batch:   40 /  113], loss: 0.343\n",
      "[Epoch: 9, Batch:   50 /  113], loss: 0.463\n",
      "[Epoch: 9, Batch:   60 /  113], loss: 0.534\n",
      "[Epoch: 9, Batch:   70 /  113], loss: 0.623\n",
      "[Epoch: 9, Batch:   80 /  113], loss: 0.547\n",
      "[Epoch: 9, Batch:   90 /  113], loss: 0.348\n",
      "[Epoch: 9, Batch:  100 /  113], loss: 0.391\n",
      "[Epoch: 9, Batch:  110 /  113], loss: 0.327\n",
      "Epoch 9 finished in 416.87 seconds. Training Accuracy: 89.78 %\n",
      "Valid accuracy: 89.75 %\n",
      "Best model saved with accuracy: 89.75 %\n",
      "[Epoch: 10, Batch:   10 /  113], loss: 0.388\n",
      "[Epoch: 10, Batch:   20 /  113], loss: 0.462\n",
      "[Epoch: 10, Batch:   30 /  113], loss: 0.436\n",
      "[Epoch: 10, Batch:   40 /  113], loss: 0.428\n",
      "[Epoch: 10, Batch:   50 /  113], loss: 0.377\n",
      "[Epoch: 10, Batch:   60 /  113], loss: 0.501\n",
      "[Epoch: 10, Batch:   70 /  113], loss: 0.441\n",
      "[Epoch: 10, Batch:   80 /  113], loss: 0.383\n",
      "[Epoch: 10, Batch:   90 /  113], loss: 0.288\n",
      "[Epoch: 10, Batch:  100 /  113], loss: 0.433\n",
      "[Epoch: 10, Batch:  110 /  113], loss: 0.345\n",
      "Epoch 10 finished in 419.42 seconds. Training Accuracy: 89.56 %\n",
      "Valid accuracy: 80.50 %\n",
      "[Epoch: 11, Batch:   10 /  113], loss: 0.358\n",
      "[Epoch: 11, Batch:   20 /  113], loss: 0.375\n",
      "[Epoch: 11, Batch:   30 /  113], loss: 0.340\n",
      "[Epoch: 11, Batch:   40 /  113], loss: 0.346\n",
      "[Epoch: 11, Batch:   50 /  113], loss: 0.441\n",
      "[Epoch: 11, Batch:   60 /  113], loss: 0.416\n",
      "[Epoch: 11, Batch:   70 /  113], loss: 0.353\n",
      "[Epoch: 11, Batch:   80 /  113], loss: 0.299\n",
      "[Epoch: 11, Batch:   90 /  113], loss: 0.392\n",
      "[Epoch: 11, Batch:  100 /  113], loss: 0.328\n",
      "[Epoch: 11, Batch:  110 /  113], loss: 0.486\n",
      "Epoch 11 finished in 421.74 seconds. Training Accuracy: 90.95 %\n",
      "Valid accuracy: 87.00 %\n",
      "[Epoch: 12, Batch:   10 /  113], loss: 0.373\n",
      "[Epoch: 12, Batch:   20 /  113], loss: 0.403\n",
      "[Epoch: 12, Batch:   30 /  113], loss: 0.301\n",
      "[Epoch: 12, Batch:   40 /  113], loss: 0.301\n",
      "[Epoch: 12, Batch:   50 /  113], loss: 0.312\n",
      "[Epoch: 12, Batch:   60 /  113], loss: 0.364\n",
      "[Epoch: 12, Batch:   70 /  113], loss: 0.378\n",
      "[Epoch: 12, Batch:   80 /  113], loss: 0.301\n",
      "[Epoch: 12, Batch:   90 /  113], loss: 0.415\n",
      "[Epoch: 12, Batch:  100 /  113], loss: 0.504\n",
      "[Epoch: 12, Batch:  110 /  113], loss: 0.315\n",
      "Epoch 12 finished in 420.69 seconds. Training Accuracy: 90.59 %\n",
      "Valid accuracy: 80.25 %\n",
      "[Epoch: 13, Batch:   10 /  113], loss: 0.322\n",
      "[Epoch: 13, Batch:   20 /  113], loss: 0.264\n",
      "[Epoch: 13, Batch:   30 /  113], loss: 0.355\n",
      "[Epoch: 13, Batch:   40 /  113], loss: 0.371\n",
      "[Epoch: 13, Batch:   50 /  113], loss: 0.246\n",
      "[Epoch: 13, Batch:   60 /  113], loss: 0.400\n",
      "[Epoch: 13, Batch:   70 /  113], loss: 0.456\n",
      "[Epoch: 13, Batch:   80 /  113], loss: 0.383\n",
      "[Epoch: 13, Batch:   90 /  113], loss: 0.342\n",
      "[Epoch: 13, Batch:  100 /  113], loss: 0.400\n",
      "[Epoch: 13, Batch:  110 /  113], loss: 0.393\n",
      "Epoch 13 finished in 420.59 seconds. Training Accuracy: 91.06 %\n",
      "Valid accuracy: 87.25 %\n",
      "[Epoch: 14, Batch:   10 /  113], loss: 0.474\n",
      "[Epoch: 14, Batch:   20 /  113], loss: 0.256\n",
      "[Epoch: 14, Batch:   30 /  113], loss: 0.379\n",
      "[Epoch: 14, Batch:   40 /  113], loss: 0.285\n",
      "[Epoch: 14, Batch:   50 /  113], loss: 0.281\n",
      "[Epoch: 14, Batch:   60 /  113], loss: 0.272\n",
      "[Epoch: 14, Batch:   70 /  113], loss: 0.414\n",
      "[Epoch: 14, Batch:   80 /  113], loss: 0.224\n",
      "[Epoch: 14, Batch:   90 /  113], loss: 0.325\n",
      "[Epoch: 14, Batch:  100 /  113], loss: 0.387\n",
      "[Epoch: 14, Batch:  110 /  113], loss: 0.360\n",
      "Epoch 14 finished in 413.92 seconds. Training Accuracy: 91.26 %\n",
      "Valid accuracy: 79.00 %\n",
      "[Epoch: 15, Batch:   10 /  113], loss: 0.397\n",
      "[Epoch: 15, Batch:   20 /  113], loss: 0.277\n",
      "[Epoch: 15, Batch:   30 /  113], loss: 0.377\n",
      "[Epoch: 15, Batch:   40 /  113], loss: 0.345\n",
      "[Epoch: 15, Batch:   50 /  113], loss: 0.311\n",
      "[Epoch: 15, Batch:   60 /  113], loss: 0.349\n",
      "[Epoch: 15, Batch:   70 /  113], loss: 0.358\n",
      "[Epoch: 15, Batch:   80 /  113], loss: 0.319\n",
      "[Epoch: 15, Batch:   90 /  113], loss: 0.332\n",
      "[Epoch: 15, Batch:  100 /  113], loss: 0.354\n",
      "[Epoch: 15, Batch:  110 /  113], loss: 0.310\n",
      "Epoch 15 finished in 428.13 seconds. Training Accuracy: 91.56 %\n",
      "Valid accuracy: 93.50 %\n",
      "Best model saved with accuracy: 93.50 %\n",
      "Total training time: 6921.25 seconds.\n"
     ]
    }
   ],
   "source": [
    "train(pointnet, train_loader, valid_loader, class_weights=class_weights, save=True, save_path=save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "36db2403-65ed-4a45-8cb9-c446875a3c6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_train_path_opt = './pointnet_cls_train/ModelNet10/best_pointnet10_cls_opt1.pth'\n",
    "pointnet = PointNet()\n",
    "pointnet.load_state_dict(torch.load(pre_train_path_opt))\n",
    "pointnet.eval();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "e41807c5-5a11-422e-9da4-499c399de774",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch [   1 /   15]\n",
      "Batch [   2 /   15]\n",
      "Batch [   3 /   15]\n",
      "Batch [   4 /   15]\n",
      "Batch [   5 /   15]\n",
      "Batch [   6 /   15]\n",
      "Batch [   7 /   15]\n",
      "Batch [   8 /   15]\n",
      "Batch [   9 /   15]\n",
      "Batch [  10 /   15]\n",
      "Batch [  11 /   15]\n",
      "Batch [  12 /   15]\n",
      "Batch [  13 /   15]\n",
      "Batch [  14 /   15]\n",
      "Batch [  15 /   15]\n",
      "Class 0: Total = 50, Correct = 34, Accuracy = 0.6800\n",
      "Class 1: Total = 100, Correct = 100, Accuracy = 1.0000\n",
      "Class 2: Total = 100, Correct = 97, Accuracy = 0.9700\n",
      "Class 3: Total = 86, Correct = 74, Accuracy = 0.8605\n",
      "Class 4: Total = 86, Correct = 64, Accuracy = 0.7442\n",
      "Class 5: Total = 100, Correct = 90, Accuracy = 0.9000\n",
      "Class 6: Total = 86, Correct = 75, Accuracy = 0.8721\n",
      "Class 7: Total = 100, Correct = 96, Accuracy = 0.9600\n",
      "Class 8: Total = 100, Correct = 96, Accuracy = 0.9600\n",
      "Class 9: Total = 100, Correct = 85, Accuracy = 0.8500\n",
      "Overall Accuracy: 0.8932\n"
     ]
    }
   ],
   "source": [
    "# 是对经过新增数据增强方法的test_loader进行测试的\n",
    "\n",
    "# 类别数量\n",
    "num_classes = 10 \n",
    "\n",
    "# 用于存储每类的预测和真实标签\n",
    "class_correct = [0] * num_classes\n",
    "class_total = [0] * num_classes\n",
    "\n",
    "# 全局统计\n",
    "total_correct = 0\n",
    "total_samples = 0\n",
    "\n",
    "all_preds = []\n",
    "all_labels = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for i, data in enumerate(test_loader):\n",
    "        print('Batch [%4d / %4d]' % (i + 1, len(test_loader)))\n",
    "\n",
    "        inputs, labels = data['pointcloud'].float(), data['category']\n",
    "        outputs, __, __ = pointnet(inputs.transpose(1, 2))\n",
    "        \n",
    "        # 获取预测结果\n",
    "        _, preds = torch.max(outputs.data, 1)\n",
    "        \n",
    "        # 更新所有预测和标签\n",
    "        all_preds += list(preds.numpy())\n",
    "        all_labels += list(labels.numpy())\n",
    "\n",
    "        # 统计每个类别的正确预测数和总数\n",
    "        for j in range(labels.size(0)):\n",
    "            label = labels[j].item()  # 获取当前样本的真实标签\n",
    "            class_total[label] += 1  # 该类样本总数加 1\n",
    "            class_correct[label] += (preds[j] == label).item()  # 如果预测正确，相应类别的正确预测数加 1\n",
    "            \n",
    "            # 更新全局统计\n",
    "            total_samples += 1\n",
    "            total_correct += (preds[j] == label).item()\n",
    "\n",
    "# 计算每类的准确率\n",
    "class_accuracy = [0] * num_classes\n",
    "for i in range(num_classes):\n",
    "    if class_total[i] > 0:\n",
    "        class_accuracy[i] = class_correct[i] / class_total[i]\n",
    "    else:\n",
    "        class_accuracy[i] = 0.0  # 如果该类没有样本，准确率为 0\n",
    "\n",
    "# 计算总体分类准确率\n",
    "overall_accuracy = total_correct / total_samples if total_samples > 0 else 0.0\n",
    "\n",
    "# 打印每类的测试数量和准确率\n",
    "for i in range(num_classes):\n",
    "    print(f'Class {i}: Total = {class_total[i]}, Correct = {class_correct[i]}, Accuracy = {class_accuracy[i]:.4f}')\n",
    "\n",
    "# 打印总体分类准确率\n",
    "print(f'Overall Accuracy: {overall_accuracy:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "225be14d-e74e-466e-b7f2-5a96273045d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch [   1 /   15]\n",
      "Batch [   2 /   15]\n",
      "Batch [   3 /   15]\n",
      "Batch [   4 /   15]\n",
      "Batch [   5 /   15]\n",
      "Batch [   6 /   15]\n",
      "Batch [   7 /   15]\n",
      "Batch [   8 /   15]\n",
      "Batch [   9 /   15]\n",
      "Batch [  10 /   15]\n",
      "Batch [  11 /   15]\n",
      "Batch [  12 /   15]\n",
      "Batch [  13 /   15]\n",
      "Batch [  14 /   15]\n",
      "Batch [  15 /   15]\n",
      "Class 0: Total = 50, Correct = 35, Accuracy = 0.7000\n",
      "Class 1: Total = 100, Correct = 100, Accuracy = 1.0000\n",
      "Class 2: Total = 100, Correct = 98, Accuracy = 0.9800\n",
      "Class 3: Total = 86, Correct = 72, Accuracy = 0.8372\n",
      "Class 4: Total = 86, Correct = 67, Accuracy = 0.7791\n",
      "Class 5: Total = 100, Correct = 89, Accuracy = 0.8900\n",
      "Class 6: Total = 86, Correct = 72, Accuracy = 0.8372\n",
      "Class 7: Total = 100, Correct = 96, Accuracy = 0.9600\n",
      "Class 8: Total = 100, Correct = 97, Accuracy = 0.9700\n",
      "Class 9: Total = 100, Correct = 85, Accuracy = 0.8500\n",
      "Overall Accuracy: 0.8932\n"
     ]
    }
   ],
   "source": [
    "#对初始数据增强方法处理的test_loader进行测试\n",
    "\n",
    "# 类别数量\n",
    "num_classes = 10 \n",
    "\n",
    "# 用于存储每类的预测和真实标签\n",
    "class_correct = [0] * num_classes\n",
    "class_total = [0] * num_classes\n",
    "\n",
    "# 全局统计\n",
    "total_correct = 0\n",
    "total_samples = 0\n",
    "\n",
    "all_preds = []\n",
    "all_labels = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for i, data in enumerate(test_loader):\n",
    "        print('Batch [%4d / %4d]' % (i + 1, len(test_loader)))\n",
    "\n",
    "        inputs, labels = data['pointcloud'].float(), data['category']\n",
    "        outputs, __, __ = pointnet(inputs.transpose(1, 2))\n",
    "        \n",
    "        # 获取预测结果\n",
    "        _, preds = torch.max(outputs.data, 1)\n",
    "        \n",
    "        # 更新所有预测和标签\n",
    "        all_preds += list(preds.numpy())\n",
    "        all_labels += list(labels.numpy())\n",
    "\n",
    "        # 统计每个类别的正确预测数和总数\n",
    "        for j in range(labels.size(0)):\n",
    "            label = labels[j].item()  # 获取当前样本的真实标签\n",
    "            class_total[label] += 1  # 该类样本总数加 1\n",
    "            class_correct[label] += (preds[j] == label).item()  # 如果预测正确，相应类别的正确预测数加 1\n",
    "            \n",
    "            # 更新全局统计\n",
    "            total_samples += 1\n",
    "            total_correct += (preds[j] == label).item()\n",
    "\n",
    "# 计算每类的准确率\n",
    "class_accuracy = [0] * num_classes\n",
    "for i in range(num_classes):\n",
    "    if class_total[i] > 0:\n",
    "        class_accuracy[i] = class_correct[i] / class_total[i]\n",
    "    else:\n",
    "        class_accuracy[i] = 0.0  # 如果该类没有样本，准确率为 0\n",
    "\n",
    "# 计算总体分类准确率\n",
    "overall_accuracy = total_correct / total_samples if total_samples > 0 else 0.0\n",
    "\n",
    "# 打印每类的测试数量和准确率\n",
    "for i in range(num_classes):\n",
    "    print(f'Class {i}: Total = {class_total[i]}, Correct = {class_correct[i]}, Accuracy = {class_accuracy[i]:.4f}')\n",
    "\n",
    "# 打印总体分类准确率\n",
    "print(f'Overall Accuracy: {overall_accuracy:.4f}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
